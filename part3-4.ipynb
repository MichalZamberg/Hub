{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb01d33b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963639e4",
   "metadata": {},
   "source": [
    "First, lets import the classes you defined in the previous parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdaa3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f629b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from model import Model\n",
    "from dataset import CellsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075f2f40",
   "metadata": {},
   "source": [
    "Now we will again read the csv with the cells' locations and filter cells on the borders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29abfe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"cells_locations.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77941a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_px = 60\n",
    "image_size = 2048\n",
    "data = data[(data[\"centroid-0\"] > size_px) & (data[\"centroid-0\"] < (image_size-size_px))]\n",
    "data = data[(data[\"centroid-1\"] > size_px) & (data[\"centroid-1\"] < (image_size-size_px))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6555fba",
   "metadata": {},
   "source": [
    "Split the dataset to training, validation and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d23c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = data.image_id.unique()\n",
    "images_train = images[:9]\n",
    "images_val = images[9:14]\n",
    "images_test = images[14:]\n",
    "data_train = data[data[\"image_id\"].isin(images_train)]\n",
    "data_val = data[data[\"image_id\"].isin(images_val)]\n",
    "data_test = data[data[\"image_id\"].isin(images_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a475a00f",
   "metadata": {},
   "source": [
    "Note that in the code above we added a validation set, it's a common practice in machine learning. \n",
    "We are training the model on the training set, making sure the model has good results and converge on the validation set. (On this set we'll also tune the hyper parameters).\n",
    "Then the final testing of the model will be on the testing set. It's done to ensure that we are not overfitting the test. And that we will eventually have good results on an unseen test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f389eae2",
   "metadata": {},
   "source": [
    "## Part 3: Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4cc386",
   "metadata": {},
   "source": [
    "Now we are going to train the model. The basic componnent of most training optimization are:\n",
    "<div>\n",
    "<ul>\n",
    "    <li><b>Model</b> - The model we are trying to optimize, this would be the CNN model we defined in previous sections</li>\n",
    "    <li><b>Dataset and Dataloader</b> - As outlined in previous sections, the DataLoader will allow us to iterate through the data in batches</li>\n",
    "    <li><b>Loss function</b> - The function we are trying to optimize. The main concept of training the neural network is adjusting the model's weights in a direction that decrease the overall loss across all the data. Lower values signify better performance in the task.\n",
    "<br>&ensp;&ensp;For example, in the task of image reconstraction one possible loss metric could be MSE, the MSE calculate the mean squared error between the real image and the reconstructed image. We want to find the weights that will give us minimum value for this loss. \n",
    "<br>&ensp;&ensp;In our case we have a classification task (involving multiple classes). The most commonly used loss is <b>CrossEntropyLoss</b>. \n",
    "<br> With CrossEntropyLoss we want to get <b>high probabilities for the correct class</b>. Essentially, the loss will be <b>-log(p)</b> where p stands for the probability of belonging to the correct class. \n",
    "<br> A high p corresponds to a low loss, while a low p leads to a high loss</li>\n",
    "    <li><b>Optimizer</b> - like many optimization problem we need to define the method in which the weights of our model will be optimized. The most known one is <b>SGD</b>. Other common method is <b>Adam</b>. <br> Those optimization methods are gradients based.</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "This was just a brief explanation, You're encouraged to explore the mathematical rationale behind it. In the first section, you were presented with a few good sources for learning it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b33f9d2",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 20px;color:red;background:#D3D3D3\"> &#187; Coding Task! </div> \n",
    "Fill in the code below, according to the comments. For each comment you should only write 1-2 lines of code.\n",
    "\n",
    "You can go over this link for help if needed:\n",
    "https://pytorch.org/tutorials/beginner/introyt/trainingyt.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80df0b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389cd71e-60e4-43d2-908e-7122017bfb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b547f391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model assuming you have 15 (+1 for the unidentified class we added) classes and 25 channels + 1 channel of segmentation:\n",
    "num_channels = 25 + 1\n",
    "class_num = 15 + 1\n",
    "model = Model(num_channels, class_num)\n",
    "\n",
    "# >> Change state of model to \"cuda\", so the calculationg will run on the GPU:\n",
    "\n",
    "# >> Change state of model to \"train\", check online what does it mean:\n",
    "\n",
    "# >> Define an optimizer, keep the defualt parameters:\n",
    "\n",
    "# >> Define the training and validation datasets\n",
    "\n",
    "# >> Define the loss function, it should be CrossEntropyLoss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbb6ab4",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 20px;color:green;background:#D3D3D3\"> &#187; Checking for understanding, Answer the following: </div> \n",
    "\n",
    "- What does model.train() do? look online to find the answer.\n",
    "- What will model.eval() do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e17e3ac",
   "metadata": {},
   "source": [
    "### Balancing the classes in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff93998d",
   "metadata": {},
   "source": [
    "Some datasets are highly un-balanced, for example this dataset. You'll notice that some cell types consist of only a few hunderds of cells while other cell types consist thousands of cells.\n",
    "For further investigation:\n",
    "<br>&ensp;&ensp; - Calculate the cell types distribution\n",
    "<br>&ensp;&ensp; - What do you see? What are the common cell types?\n",
    "<br>&ensp;&ensp; - Balance the dataset so that in each batch the model will see equal proportion of each cell type. (check \"WeightedRandomSampler\" from pytorch, note that it's expecting to get the weights for each sample and not the weights of each class)\n",
    "\n",
    "Note that it'll require you to also define the trainloader here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b6583d",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 20px;color:red;background:#D3D3D3\"> &#187; Coding Task! </div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818c7df4-49ec-42e5-b83d-e7658e9fba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wite code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19b2e9c",
   "metadata": {},
   "source": [
    "We'll now test that during one pass over the data we get approximately equal proportion of each cell type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442d6dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for _, _, label in train_loader:\n",
    "    labels += label.tolist()\n",
    "labels = np.array(labels)\n",
    "_, count_unique = np.unique(labels, return_counts=True)\n",
    "np.testing.assert_allclose(count_unique - count_unique.mean(), np.zeros_like(count_unique), atol=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7530e01f",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 20px;color:red;background:#D3D3D3\"> &#187; Coding Task! </div> \n",
    "Define the validation Dataloader here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c35102",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_dataset, \n",
    "                          batch_size=32,\n",
    "                          num_workers=4,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f3d7a7-2d60-4926-b7b0-6f95efd582fa",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e843562-e847-4a64-8f03-3a47b14cfe7a",
   "metadata": {},
   "source": [
    "As you may recall, we've split the data to: train, validation and test. The training loop iterates over the training data to optimize the model accordingly. <br>\n",
    "For each training epoch, we want to evaluate the model on the validation set and record the validation loss. This will allow us to monitor the training process and make ensure that we are not overfitting the training set. <br>\n",
    "In the next section you will first implement the training loop.\n",
    "Afterward you will implement the validation step. <br>\n",
    "Finally you will train and observe the model by comparing the training and validation loss in each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3451fc79",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b71ace5",
   "metadata": {},
   "source": [
    "Now we will create the training loop, for each epoch we want to iterate over the whole dataset and update the weights of the model.\n",
    "In general one iteration over a batch will have the following steps:\n",
    "- Zero the gradients (from previous iterations)\n",
    "- Prepare the input for the network: you should concatenate the protein channels with the segmentation channel.\n",
    "- Run the model \n",
    "- Calculate the loss\n",
    "- Propogate the gradients back from the loss back thourgh the model.\n",
    "- Do one step of optimization based on the gradient pytorch calculate for us.\n",
    "\n",
    "A few notes before you start:\n",
    "*   Make sure to run on cuda to save time\n",
    "*   Note that the first channel in the input should be the channels for the model.\n",
    "*   You might want to save your model so you can reload it after training, see more here: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "*   <b>A basic technique for debugging deep learning code is attempting to overfit your training data. To achieve this, you should train on small, consistent subset of your data for multiple epochs. Ensure that the loss decrease and that accuracy reaches high values. Begin with attempting to overfit on just 10 to 100 cells.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df15b0eb",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 20px;color:red;background:#D3D3D3\"> &#187; Coding Task! </div> \n",
    "Fill in the function below, according to the comments. For each comment you should only write 1-2 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38be3004-141f-4453-af26-58b635803dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, train_loader):\n",
    "    train_losses = []\n",
    "    model.train() #Notice we are changing the model state to train mode.\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        imgs, seg, gts = batch\n",
    "        imgs = imgs.to(device=device)\n",
    "        gts = gts.to(device=device).long()\n",
    "        imgs = torch.moveaxis(imgs, -1, 1).float() # Matching the order of the axes from the DataLoader to the model's expectations.\n",
    "        \n",
    "        #>> Prepare the input to the model\n",
    "        \n",
    "        # >> Zero the gradients of the optimizer\n",
    "        \n",
    "        # >> Run the model on the imgs\n",
    "\n",
    "        # >> Calculate the loss\n",
    "        # l = ...\n",
    "        \n",
    "        # >> Append the training loss to the train_losses\n",
    "\n",
    "        # >> Propogate back the gradients from the loss to the input\n",
    "\n",
    "        # >> Apply one step of optimization the oprimixation function.  \n",
    "        \n",
    "    return np.mean(train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24765f2-245e-4a11-8f88-95bc3d0d2bb4",
   "metadata": {},
   "source": [
    "### The validation step\n",
    "\n",
    "Now we will implement the validation step, for each epoch we want to iterate over the whole validation dataset and calculate the loss of the dataset, without optimizing the model. we just want to measure how good the model generalize.\n",
    "\n",
    "A few notes before you start:\n",
    "*   Make sure to run on cuda to save time\n",
    "*   Note that the first channel in the input should be the channels for the model.\n",
    "*   Remember to use \"with torch.no_grad():\" before running the model in order to make the calculation more efficient. When we are evaluating the model there is no need to keep track of the gradients, <a href=\"https://pytorch.org/docs/stable/generated/torch.no_grad.html\">See doc</a>.\n",
    "\n",
    "<div style=\"font-size: 20px;color:red;background:#D3D3D3\"> &#187; Coding Task! </div> \n",
    "Fill in the function below, according to the comments. For each comment you should only write 1-2 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11726c0e-ba78-417f-a595-d10400787322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(model, val_loader):\n",
    "    model.eval() #Notice we are changing the model state to eval mode.\n",
    "    val_losses = []\n",
    "    for i, batch in enumerate(val_loader):\n",
    "        imgs, seg, gts = batch\n",
    "        imgs = imgs.to(device=device)\n",
    "        gts = gts.to(device=device).long()\n",
    "        imgs = torch.moveaxis(imgs, -1, 1).float()\n",
    "        #>> Prepare the input to the model\n",
    "\n",
    "        # >> Run the model on the batch with no_softmax=True\n",
    "\n",
    "        \n",
    "        # >> Calculate the validation loss and append it to val_losses\n",
    "\n",
    "    return np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6981456a-3c79-4fa5-aaf4-5575a1dc8e0d",
   "metadata": {},
   "source": [
    "### The whole training and validation process:\n",
    "Finally, we'll integrate the training loop and the validation step so that after each training epoch, we perform one evaluation pass on the validation set.\n",
    " <br>\n",
    "Make sure to present the validation loss as well as the training loss on the lossplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ac6b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from livelossplot import PlotLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6260900",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e31893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "#Relevant only for part6\n",
    "augmentations = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19802f26-62be-42ce-95bc-7d3db2f6094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "liveloss = PlotLosses()\n",
    "for e in tqdm.tqdm(range(epochs)):\n",
    "    train_loss_avg = training_loop(model, train_loader)\n",
    "    val_loss_avg = validation_step(model, val_loader)\n",
    "    torch.save(model.state_dict(), f\"checkpoint_weights_{e}.pth\")\n",
    "\n",
    "    liveloss.update({\"train_losses\": train_loss_avg,\n",
    "                \"val_losses\": val_loss_avg})\n",
    "    liveloss.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237e222f",
   "metadata": {},
   "source": [
    "To assess the training, we need to observe the decreasing loss and validate its progress.\n",
    "Keep track of the losse's values above. Train for at least 10 epochs.\n",
    "<div style=\"font-size: 20px;color:green;background:#D3D3D3\"> &#187; Checking for understanding, Answer the following: </div> \n",
    "\n",
    "- Did you managed to overfit on the small subset from the training set? \n",
    "- Describe the bahvior of the training and validation loss as you overfit on increaing training set, is it as expected?\n",
    "- What impact does the learning rate have? Try various values and observe the change in the loss behavior.\n",
    "- What do you think will be the impact of changing the batch size?\n",
    "- How will you choose the stopping epoch for trainings? Can you recognize the point of overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc27eead",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 20px;color:blue;background:#D3D3D3\"> &#187; Important! </div> \n",
    "\n",
    "Running on the whole dataset for 10 epochs should take around 15 minutes, you can definitely train your own model but if you want you can load the pretrained model from \"checkpoint.pth\" so you wouldn't have to wait for training on the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99a6bea",
   "metadata": {},
   "source": [
    "## Part 4: Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37c34f6",
   "metadata": {},
   "source": [
    "We now aim to assess our model's performance on the test set. The motivation for this step is to asses how well our model generalizes to unseen data. <br> In order to be unbiased, we've selected a test set originating from different images than the training data. <br>\n",
    "In the next section, we will define metrics for evaluating the model. However, in this section, we are just going to loop over the test set and record the predictions provided by the model.\n",
    "\n",
    "Let's start by creating a DataLoader for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb400752",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CellsDataset(data_test)\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                          batch_size=32,\n",
    "                          num_workers=2,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a18601",
   "metadata": {},
   "source": [
    "Let's load a model that was trained with the complete training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff64fb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load weights from checkpoint.pth\n",
    "model.load_state_dict(torch.load(\"checkpoint.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d3925",
   "metadata": {},
   "source": [
    "Change the state of the model to eval:\n",
    "<br>\n",
    "<i>(The <b>;</b> is  here just to prevent the function from printing to the notebook)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86696d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6273dc55",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"font-size: 20px;color:red;background:#D3D3D3\"> &#187; Coding Task! </div> \n",
    "\n",
    "Now we'll loop over the DataLoader and apply the model. <br>\n",
    "Change the code below and accumulate the gts and the predictions in each iteration, save them as two lists, in the following cell we'll create a DataFrame out of those lists:\n",
    "\n",
    "Remember to use \"with torch.no_grad():\" before running the model in order to make the calculation more efficient. \n",
    "When we are evaluating the model there is no need to keep track of the gradients, <a href=\"https://pytorch.org/docs/stable/generated/torch.no_grad.html\">See doc</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb17f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "gts_total = []\n",
    "preds_total = []\n",
    "for i, batch in enumerate(test_loader):\n",
    "    imgs,seg, gts = batch\n",
    "    imgs = imgs.to(device=device)\n",
    "    imgs = torch.moveaxis(imgs, -1, 1).float()\n",
    "    gts = gts.to(device=device).long()\n",
    "    #>> Prepare the input to the model\n",
    "\n",
    "    # >> Run the model and keep the gts and predictions\n",
    "\n",
    "    \n",
    "    gts_total += gts.flatten().tolist()\n",
    "    preds_total += preds.argmax(axis=1).flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fc1be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\"gts\": gts_total, \"preds\": preds_total})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d96d921",
   "metadata": {},
   "source": [
    "Save the results for the next part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a9e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4d511a",
   "metadata": {},
   "source": [
    "In the next sections, we will measure the model's performance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aihub_env",
   "language": "python",
   "name": "aihub_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
